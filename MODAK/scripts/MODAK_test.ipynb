{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the MODAK driver program\n"
     ]
    }
   ],
   "source": [
    "from MODAK_driver import MODAK_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8a3fb0472fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODAK_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/sodalite/MODAK/src/MODAK_driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, conf_file, install)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MODAK\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqlContextHive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiveContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_IaC_modelrepo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUITE_SERVER_LOGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quiet_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/sodalite/MODAK/src/MODAK_driver.py\u001b[0m in \u001b[0;36m__init_IaC_modelrepo\u001b[0;34m(self, install)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySQL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODAK_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_create_stmt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDB_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdfdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqlContextHive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'show databases'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mdfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'namespace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/build/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/build/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \"\"\"\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/build/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/build/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;"
     ]
    }
   ],
   "source": [
    "driver = MODAK_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = '/Users/ksivalinga/Documents/sodalite/iac-model-repo/db/test-iac-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = driver.applySQL(\"CREATE DATABASE `iac_model`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = driver.applySQL(\"CREATE DATABASE `test_iac_model`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|       default|\n",
      "|     iac_model|\n",
      "|test_iac_model|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('show databases')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = driver.applySQL('use test_iac_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----------+\n",
      "|      database|     tableName|isTemporary|\n",
      "+--------------+--------------+-----------+\n",
      "|test_iac_model|infrastructure|      false|\n",
      "+--------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('show tables')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.applySQL('drop table infrastructure')\n",
    "CREATE_INFRA_TABLE = \"create external table          \\\n",
    "        infrastructure(infra_id  int,                \\\n",
    "        name string,                                 \\\n",
    "        num_nodes int,                               \\\n",
    "        is_active boolean,                           \\\n",
    "        description string)                          \\\n",
    "        stored as PARQUET location                   \\\n",
    "        '{}/infrastructure'\"\n",
    "driver.applySQL(CREATE_INFRA_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0e6433e35018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySQL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'desc infrastructure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc infrastructure')\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL(\"insert into infrastructure select 1 as infra_id,'test' as name,2 as num_nodes,\\\n",
    "TRUE as is_active, 'test' as description\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table queue')\n",
    "CREATE_QUEUE_TABLE = \"create external table          \\\n",
    "        queue(queue_id  int,                         \\\n",
    "        name string,                                 \\\n",
    "        num_nodes int,                               \\\n",
    "        is_active boolean,                           \\\n",
    "        node_spec string,                            \\\n",
    "        description string,                          \\\n",
    "        infra_id int)                                \\\n",
    "        stored as parquet location                   \\\n",
    "        '{}/queue'\"\n",
    "driver.applySQL(CREATE_QUEUE_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|   queue_id|      int|   null|\n",
      "|       name|   string|   null|\n",
      "|  num_nodes|      int|   null|\n",
      "|  is_active|  boolean|   null|\n",
      "|  node_spec|   string|   null|\n",
      "|description|   string|   null|\n",
      "|   infra_id|      int|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc queue')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table benchmark')\n",
    "CREATE_BENCH_TABLE = \"create external table          \\\n",
    "        benchmark(run_id int,                        \\\n",
    "        queue_id  int,                               \\\n",
    "        num_cores int,                               \\\n",
    "        compute_flops double,                        \\\n",
    "        memory_bw double,                            \\\n",
    "        network_bw double,                           \\\n",
    "        io_bw double,                                \\\n",
    "        acc_compute_flops double,                    \\\n",
    "        acc_memory_bw double,                        \\\n",
    "        PCIe_bw double)                              \\\n",
    "        stored as parquet location                   \\\n",
    "        '{}/benchmark'\"\n",
    "driver.applySQL(CREATE_BENCH_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|           run_id|      int|   null|\n",
      "|         queue_id|      int|   null|\n",
      "|        num_cores|      int|   null|\n",
      "|    compute_flops|   double|   null|\n",
      "|        memory_bw|   double|   null|\n",
      "|       network_bw|   double|   null|\n",
      "|            io_bw|   double|   null|\n",
      "|acc_compute_flops|   double|   null|\n",
      "|    acc_memory_bw|   double|   null|\n",
      "|          PCIe_bw|   double|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc benchmark')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table model')\n",
    "CREATE_MODEL_TABLE = \"create external table          \\\n",
    "        model(model_id int,                          \\\n",
    "        queue_id  int,                               \\\n",
    "        compute_flops string,                     \\\n",
    "        memory_bw string,                            \\\n",
    "        network_bw string,                           \\\n",
    "        io_bw string,                                \\\n",
    "        acc_compute_flops string,                    \\\n",
    "        acc_memory_bw string,                        \\\n",
    "        PCIe_bw string)                              \\\n",
    "        stored as parquet location                   \\\n",
    "        '{}/model'\"\n",
    "driver.applySQL(CREATE_MODEL_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|         model_id|      int|   null|\n",
      "|         queue_id|      int|   null|\n",
      "|    compute_flops|   string|   null|\n",
      "|        memory_bw|   string|   null|\n",
      "|       network_bw|   string|   null|\n",
      "|            io_bw|   string|   null|\n",
      "|acc_compute_flops|   string|   null|\n",
      "|    acc_memory_bw|   string|   null|\n",
      "|          PCIe_bw|   string|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc model')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table appmodel')\n",
    "CREATE_APPMODEL_TABLE = \"create external table       \\\n",
    "        appmodel(appmodel_id int,                       \\\n",
    "        queue_id  int,                               \\\n",
    "        app_id  int,                                 \\\n",
    "        compute_flops double,                        \\\n",
    "        memory_bw double,                            \\\n",
    "        network_bw double,                           \\\n",
    "        io_bw double,                                \\\n",
    "        acc_compute_flops double,                    \\\n",
    "        acc_memory_bw double,                        \\\n",
    "        PCIe_bw double,                              \\\n",
    "        acc_share double)                            \\\n",
    "        stored as parquet location                   \\\n",
    "        '{}/appmodel'\"\n",
    "driver.applySQL(CREATE_APPMODEL_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|      appmodel_id|      int|   null|\n",
      "|         queue_id|      int|   null|\n",
      "|           app_id|      int|   null|\n",
      "|    compute_flops|   double|   null|\n",
      "|        memory_bw|   double|   null|\n",
      "|       network_bw|   double|   null|\n",
      "|            io_bw|   double|   null|\n",
      "|acc_compute_flops|   double|   null|\n",
      "|    acc_memory_bw|   double|   null|\n",
      "|          PCIe_bw|   double|   null|\n",
      "|        acc_share|   double|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc appmodel')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table application')\n",
    "CREATE_APP_TABLE = \"create external table            \\\n",
    "        application(app_id int,                      \\\n",
    "        name  string,                                \\\n",
    "        app_type string,                             \\\n",
    "        description string,                          \\\n",
    "        src string)                                  \\\n",
    "        stored as parquet location                   \\\n",
    "        '{}/application'\"\n",
    "driver.applySQL(CREATE_APP_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|     app_id|      int|   null|\n",
      "|       name|   string|   null|\n",
      "|   app_type|   string|   null|\n",
      "|description|   string|   null|\n",
      "|        src|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc application')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table audit_log')\n",
    "CREATE_AUDIT_TABLE = \"create external table         \\\n",
    "        audit_log(file_line  bigint,               \\\n",
    "        start_time timestamp,                       \\\n",
    "        end_time timestamp,                         \\\n",
    "        run_time_sec bigint,                        \\\n",
    "        queue_id  int,                              \\\n",
    "        app_id  int,                                \\\n",
    "        aprun_id bigint,                            \\\n",
    "        job_id string,                              \\\n",
    "        num_nodes int,                              \\\n",
    "        run_stat int,                               \\\n",
    "        command string,                             \\\n",
    "        command_uniq string)                        \\\n",
    "        stored as parquet location                  \\\n",
    "        '{}/audit_log'\"\n",
    "driver.applySQL(CREATE_AUDIT_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|   file_line|   bigint|   null|\n",
      "|  start_time|timestamp|   null|\n",
      "|    end_time|timestamp|   null|\n",
      "|run_time_sec|   bigint|   null|\n",
      "|    queue_id|      int|   null|\n",
      "|      app_id|      int|   null|\n",
      "|    aprun_id|   bigint|   null|\n",
      "|      job_id|   string|   null|\n",
      "|   num_nodes|      int|   null|\n",
      "|    run_stat|      int|   null|\n",
      "|     command|   string|   null|\n",
      "|command_uniq|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc audit_log')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table optimisation')\n",
    "CREATE_OPT_TABLE = \"create external table            \\\n",
    "        optimisation(opt_id int,                      \\\n",
    "        opt_dsl_code  string,                                \\\n",
    "        app_name  string,                                \\\n",
    "        target string,                                  \\\n",
    "        optimisation string)                                  \\\n",
    "        stored as parquet location                   \\\n",
    "        '{}/optimisation'\"\n",
    "driver.applySQL(CREATE_OPT_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|      opt_id|      int|   null|\n",
      "|opt_dsl_code|   string|   null|\n",
      "|    app_name|   string|   null|\n",
      "|      target|   string|   null|\n",
      "|optimisation|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc optimisation')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.applySQL('drop table mapper')\n",
    "CREATE_MAPPER_TABLE = \"create external table            \\\n",
    "        mapper(map_id int,                      \\\n",
    "        opt_dsl_code  string,                                \\\n",
    "        container_file string,                             \\\n",
    "        image_type string,                                  \\\n",
    "        image_hub string,                                  \\\n",
    "        src string)                                        \\\n",
    "        stored as parquet location                   \\\n",
    "        '{}/mapper'\"\n",
    "driver.applySQL(CREATE_MAPPER_TABLE.format(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------+\n",
      "|      col_name|data_type|comment|\n",
      "+--------------+---------+-------+\n",
      "|        map_id|      int|   null|\n",
      "|  opt_dsl_code|   string|   null|\n",
      "|container_file|   string|   null|\n",
      "|    image_type|   string|   null|\n",
      "|     image_hub|   string|   null|\n",
      "|           src|   string|   null|\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL('desc mapper')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = driver.applySQL(\"SELECT * FROM mapper\")\n",
    "df.coalesce(1).write.option(\"header\", \"true\").format(\"com.databricks.spark.csv\").save(\"/Users/ksivalinga/Documents/CERL/sodalite/test-iac-model/mapper.csv\")\n",
    "df = driver.applySQL(\"SELECT * FROM optimisation\")\n",
    "df.coalesce(1).write.option(\"header\", \"true\").format(\"com.databricks.spark.csv\").save(\"/Users/ksivalinga/Documents/CERL/sodalite/test-iac-model/optimisation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+------+------------+\n",
      "|opt_id|opt_dsl_code|app_name|target|optimisation|\n",
      "+------+------------+--------+------+------------+\n",
      "+------+------------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver.applySQL('select * from optimisation').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.applySQL('alter table mapper concatenate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mapper main\n",
      "+------+\n",
      "|opt_id|\n",
      "+------+\n",
      "|     9|\n",
      "+------+\n",
      "\n",
      "9\n",
      "INSERT INTO optimisation select 10 as opt_id, 'TF_PIP_XLA' as opt_dsl_code,                         'tensorflow' as app_name, 'cpu_type:x86,acc_type:nvidia' as target, 'xla:true,version:1.1' as optimisation\n"
     ]
    }
   ],
   "source": [
    "from mapper import mapper\n",
    "m = mapper(driver)\n",
    "print('Test mapper main')\n",
    "# m.addContainer('TF_PIP_XLA','AI/containers/tensorflow/tensorflow_pip_xla','Docker')\n",
    "m.add_optimisation('TF_PIP_XLA','tensorflow','cpu_type:x86,acc_type:nvidia','xla:true,version:1.1')\n",
    "# print(m.getContainer('TF_PIP_XLA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|opt_id|\n",
      "+------+\n",
      "|     8|\n",
      "+------+\n",
      "\n",
      "8\n",
      "INSERT INTO optimisation select 9 as opt_id, 'TF_PIP_XLA' as opt_dsl_code,                         'tensorflow' as app_name, 'cpu_type:x86,acc_type:nvidia' as target, 'xla:true,version:1.1' as optimisation, '2.1' as version\n"
     ]
    }
   ],
   "source": [
    "m.add_optimisation(opt_dsl_code='TF_PIP_XLA',app_name='tensorflow',target='cpu_type:x86,acc_type:nvidia',optimisation='xla:true,version:1.1',version='2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|opt_dsl_code|\n",
      "+------------+\n",
      "|  TF_PIP_XLA|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = driver.applySQL(\"select opt_dsl_code from optimisation where app_name = 'tensorflow'\\\n",
    "and target like '%cpu_type:x86%' and target like '%acc_type:nvidia%' and \\\n",
    "optimisation like '%version:1.1%' and optimisation like '%xla:true%'\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select container_file, image_hub from mapper                                   where opt_dsl_code='TF_PIP_XLA' order by map_id desc limit 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Docker/AI/containers/tensorflow/tensorflow_pip_xla'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_dsl_code = 'TF_PIP_XLA'\n",
    "stmt = \"select container_file, image_hub from mapper \\\n",
    "                                  where opt_dsl_code='{}' order by map_id desc limit 1\"\n",
    "print(stmt.format(opt_dsl_code))\n",
    "df = driver.applySQL(stmt.format(opt_dsl_code))\n",
    "container_file = df.select('container_file').collect()[0][0]\n",
    "image_hub = df.select('image_hub').collect()[0][0]\n",
    "str(image_hub) + str('/')+str(container_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load('/Users/ksivalinga/Documents/sodalite/iac-model-repo/db/data/mapper.csv')\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"mapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = driver.applySQL(\"insert overwrite mapper select * from csv.`/Users/ksivalinga/Documents/sodalite/iac-model-repo/db/data/mapper.csv`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+----------+---------+----+\n",
      "|map_id|        opt_dsl_code|      container_file|image_type|image_hub| src|\n",
      "+------+--------------------+--------------------+----------+---------+----+\n",
      "|     1|modak-pytorch-1.5...|modakopt/modak:py...|    docker|   docker|none|\n",
      "|     2|modak-pytorch-1.5...|modakopt/modak:py...|    docker|   docker|none|\n",
      "|     3|modak-pytorch-1.5...|modakopt/modak:py...|    docker|   docker|none|\n",
      "|     4|modak-pytorch-1.5...|modakopt/modak:py...|    docker|   docker|none|\n",
      "|     5|modak-pytorch-lat...|modakopt/modak:py...|    docker|   docker|none|\n",
      "|     6|modak-pytorch-lat...|modakopt/modak:py...|    docker|   docker|none|\n",
      "|     7|modak-pytorch-lat...|modakopt/modak:py...|    docker|   docker|none|\n",
      "|     8|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|     9|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    10|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    11|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    12|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    13|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    14|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    15|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    16|modak-tensorflow-...|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    17|   modak-test-ubuntu|modakopt/modak:te...|    docker|   docker|none|\n",
      "|    18|  modak-ubuntu-18-04|modakopt/modak:ub...|    docker|   docker|none|\n",
      "|    19|tensorflow-latest...|tensorflow/tensor...|    docker|   docker|none|\n",
      "|    20|tensorflow-latest...|tensorflow/tensor...|    docker|   docker|none|\n",
      "+------+--------------------+--------------------+----------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver.applySQL(\"select * from mapper\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"false\").option(\"sep\",\",\").load('/Users/ksivalinga/Documents/sodalite/iac-model-repo/db/data/optimisation.csv')\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"optimisation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = driver.applySQL(\"insert overwrite optimisation select * from csv.`/Users/ksivalinga/Documents/sodalite/iac-model-repo/db/data/optimisation.csv`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+--------------------+--------------------+\n",
      "|opt_id|        opt_dsl_code|  app_name|              target|        optimisation|\n",
      "+------+--------------------+----------+--------------------+--------------------+\n",
      "|     1|modak-pytorch-1.5...|   pytorch|enable_opt_build:...|        version:1.5||\n",
      "|     2|modak-pytorch-1.5...|   pytorch|enable_opt_build:...|        version:1.5||\n",
      "|     3|modak-pytorch-1.5...|   pytorch|enable_opt_build:...|        version:1.5||\n",
      "|     4|modak-pytorch-1.5...|   pytorch|enable_opt_build:...|        version:1.5||\n",
      "|     5|modak-pytorch-lat...|   pytorch|enable_opt_build:...|     version:latest||\n",
      "|     6|modak-pytorch-lat...|   pytorch|enable_opt_build:...|     version:latest||\n",
      "|     7|modak-pytorch-lat...|   pytorch|enable_opt_build:...|     version:latest||\n",
      "|     8|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|     9|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    10|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    11|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    12|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    13|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    14|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    15|modak-tensorflow-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    16|modak-tensorflow-...|tensorflow|enable_opt_build:...|ngraph:true|versi...|\n",
      "|    17|   modak-test-ubuntu|      test|enable_opt_build:...|           test:true|\n",
      "|    18|  modak-ubuntu-18-04|    ubuntu|enable_opt_build:...|                null|\n",
      "|    19|tensorflow-latest...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    20|tensorflow-latest...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    21|tensorflow-2.1.0-...|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    22|tensorflow-2.1.0-py3|tensorflow|enable_opt_build:...|xla:true|version:...|\n",
      "|    23|      pytorch-latest|   pytorch|enable_opt_build:...|     version:latest||\n",
      "|    24|pytorch-1.5.1-cud...|   pytorch|enable_opt_build:...|        version:1.5||\n",
      "+------+--------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver.applySQL(\"select * from optimisation\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.applySQL(\"delete from optimisation where opt_dsl_code='opt_dsl_code' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+-----------+-----------+-----------+-----------+---------+-----------+-------------+------------+---------+----------+---------------+----------+----------+------------+----------+---------------------+----------------+------------+---------------+----------------+----------------+--------------+-------------------+------------------+----------------+----------+------------+----------------------+--------+----------+-----------+------------+-------------+-----------+---------------+--------------------+--------------------+---------------------+----------------+---------------------+-----------------+--------------+----------------+--------------+----------------------+-------------------+------------------------+---------------+\n",
      "|     Host|            User|Select_priv|Insert_priv|Update_priv|Delete_priv|Create_priv|Drop_priv|Reload_priv|Shutdown_priv|Process_priv|File_priv|Grant_priv|References_priv|Index_priv|Alter_priv|Show_db_priv|Super_priv|Create_tmp_table_priv|Lock_tables_priv|Execute_priv|Repl_slave_priv|Repl_client_priv|Create_view_priv|Show_view_priv|Create_routine_priv|Alter_routine_priv|Create_user_priv|Event_priv|Trigger_priv|Create_tablespace_priv|ssl_type|ssl_cipher|x509_issuer|x509_subject|max_questions|max_updates|max_connections|max_user_connections|              plugin|authentication_string|password_expired|password_last_changed|password_lifetime|account_locked|Create_role_priv|Drop_role_priv|Password_reuse_history|Password_reuse_time|Password_require_current|User_attributes|\n",
      "+---------+----------------+-----------+-----------+-----------+-----------+-----------+---------+-----------+-------------+------------+---------+----------+---------------+----------+----------+------------+----------+---------------------+----------------+------------+---------------+----------------+----------------+--------------+-------------------+------------------+----------------+----------+------------+----------------------+--------+----------+-----------+------------+-------------+-----------+---------------+--------------------+--------------------+---------------------+----------------+---------------------+-----------------+--------------+----------------+--------------+----------------------+-------------------+------------------------+---------------+\n",
      "|localhost|mysql.infoschema|          Y|          N|          N|          N|          N|        N|          N|            N|           N|        N|         N|              N|         N|         N|           N|         N|                    N|               N|           N|              N|               N|               N|             N|                  N|                 N|               N|         N|           N|                     N|        |        []|         []|          []|            0|          0|              0|                   0|caching_sha2_pass...| $A$005$THISISACOM...|               N|  2020-07-17 20:12:28|             null|             Y|               N|             N|                  null|               null|                    null|           null|\n",
      "|localhost|   mysql.session|          N|          N|          N|          N|          N|        N|          N|            Y|           N|        N|         N|              N|         N|         N|           N|         Y|                    N|               N|           N|              N|               N|               N|             N|                  N|                 N|               N|         N|           N|                     N|        |        []|         []|          []|            0|          0|              0|                   0|caching_sha2_pass...| $A$005$THISISACOM...|               N|  2020-07-17 20:12:28|             null|             Y|               N|             N|                  null|               null|                    null|           null|\n",
      "|localhost|       mysql.sys|          N|          N|          N|          N|          N|        N|          N|            N|           N|        N|         N|              N|         N|         N|           N|         N|                    N|               N|           N|              N|               N|               N|             N|                  N|                 N|               N|         N|           N|                     N|        |        []|         []|          []|            0|          0|              0|                   0|caching_sha2_pass...| $A$005$THISISACOM...|               N|  2020-07-17 20:12:28|             null|             Y|               N|             N|                  null|               null|                    null|           null|\n",
      "|localhost|            root|          Y|          Y|          Y|          Y|          Y|        Y|          Y|            Y|           Y|        Y|         Y|              Y|         Y|         Y|           Y|         Y|                    Y|               Y|           Y|              Y|               Y|               Y|             Y|                  Y|                 Y|               Y|         Y|           Y|                     Y|        |        []|         []|          []|            0|          0|              0|                   0|mysql_native_pass...| *EE76F2E2A518BBDE...|               N|  2020-07-17 20:12:29|             null|             N|               Y|             Y|                  null|               null|                    null|           null|\n",
      "+---------+----------------+-----------+-----------+-----------+-----------+-----------+---------+-----------+-------------+------------+---------+----------+---------------+----------+----------+------------+----------+---------------------+----------------+------------+---------------+----------------+----------------+--------------+-------------------+------------------+----------------+----------+------------+----------------------+--------+----------+-----------+------------+-------------+-----------+---------------+--------------------+--------------------+---------------------+----------------+---------------------+-----------------+--------------+----------------+--------------+----------------------+-------------------+------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /Users/ksivalinga/Documents/build/mysql-connector-java-5.1.45/mysql-connector-java-5.1.45-bin.jar  pyspark-shell'\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"TestPySparkJDBC\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#Provide your Spark-master node below\n",
    "hostname = \"localhost\" \n",
    "dbname = \"mysql\"\n",
    "jdbcPort = 3306\n",
    "username = \"root\"\n",
    "password = \"Abhinaya21\"\n",
    "jdbc_url = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}\".format(hostname,jdbcPort, dbname,username,password)\n",
    "\n",
    "\n",
    "query = \"(select * from user) t1_alias\"\n",
    "df1 = sqlContext.read.format('jdbc').options(driver = 'com.mysql.jdbc.Driver',url=jdbc_url, dbtable=query ).load()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
