{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train PeakLens optimized model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"PeakLens_optimized\"\n",
    "DATASET_PATH = \"/data/skyline-extraction-patches-dataset\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=x,\n",
    "        filters=32,\n",
    "        kernel_size=[3, 3],\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    conv2 = tf.layers.separable_conv2d(\n",
    "        inputs=conv1,\n",
    "        filters=32,\n",
    "        kernel_size=[3, 3],\n",
    "        strides=(2, 2),\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    conv3 = tf.layers.separable_conv2d(\n",
    "        inputs=conv2,\n",
    "        filters=64,\n",
    "        kernel_size=[3, 3],\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    conv4 = tf.layers.separable_conv2d(\n",
    "        inputs=conv3,\n",
    "        filters=64,\n",
    "        kernel_size=[3, 3],\n",
    "        strides=(2, 2),\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    conv5 = tf.layers.separable_conv2d(\n",
    "        inputs=conv4,\n",
    "        filters=128,\n",
    "        kernel_size=[3, 3],\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    conv6 = tf.layers.conv2d(\n",
    "        inputs=conv5, \n",
    "        filters=2, \n",
    "        kernel_size=[3,3])\n",
    "\n",
    "    output = tf.reshape(conv6, [-1, 1 * 1 * 2])\n",
    "\n",
    "    softmax = tf.nn.softmax(output, name=\"softmax_tensor\")\n",
    "\n",
    "    return output, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_label(path, label):\n",
    "    image = tf.read_file(path)\n",
    "    image_decoded = tf.image.decode_jpeg(image)\n",
    "    image_decoded = tf.image.random_flip_left_right(image_decoded)    \n",
    "    return image_decoded, label\n",
    "\n",
    "def get_dataset_split(split):\n",
    "    positive_paths = glob.glob(\"{}/{}/patches/positive/*.jpg\".format(DATASET_PATH, split))\n",
    "    negative_paths = glob.glob(\"{}/{}/patches/negative/*.jpg\".format(DATASET_PATH, split))\n",
    "  \n",
    "    positive_labels = [1] * len(positive_paths)\n",
    "    negative_labels = [0] * len(negative_paths)\n",
    "  \n",
    "    paths = positive_paths + negative_paths\n",
    "    labels = positive_labels + negative_labels\n",
    "\n",
    "    tf_paths = tf.constant(paths)\n",
    "    tf_labels = tf.constant(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tf_paths, tf_labels))\n",
    "    dataset = dataset.map(load_image_and_label, num_parallel_calls=multiprocessing.cpu_count())\\\n",
    "        .shuffle(len(paths)).batch(BATCH_SIZE).repeat(EPOCHS).prefetch(2)\n",
    "    return dataset, len(paths)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training split loaded (1,699,328 images).\n",
      "Validation split loaded (424,448 images).\n",
      "Test split loaded (530,944 images).\n"
     ]
    }
   ],
   "source": [
    "training_dataset, training_steps = get_dataset_split(\"training\")\n",
    "print(\"Training split loaded ({:,} images).\".format(training_steps*BATCH_SIZE))\n",
    "\n",
    "validation_dataset, validation_steps = get_dataset_split(\"validation\")\n",
    "print(\"Validation split loaded ({:,} images).\".format(validation_steps*BATCH_SIZE))\n",
    "\n",
    "test_dataset, test_steps = get_dataset_split(\"testing\")\n",
    "print(\"Test split loaded ({:,} images).\".format(test_steps*BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 20,578.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build the graph for the deep net\n",
    "x = tf.placeholder(tf.float32, [None, 29, 29, 3])\n",
    "y_ = tf.placeholder(tf.int64, [None])\n",
    "y_conv, softmax = deepnn(x)\n",
    "\n",
    "print(\"Parameters: {:,}.\".format(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss, adam_optimizer, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cross entropy as cost function\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y_conv)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Set Adam optimizer as trainer\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), y_)\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to visualize the training with tensorboard by executing the following command with the corresponding logdir path\n",
    "\n",
    "tensorboard --logdir=graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training.\n",
      "Epoch 0, training accuracy: 0.940754\n",
      "Epoch 0, validation accuracy: 0.954395\n",
      "Epoch 0, validation error 0.122062 - session saved\n",
      "Epoch 1, training accuracy: 0.956603\n",
      "Epoch 1, validation accuracy: 0.958634\n",
      "Epoch 1, validation error 0.113437 - session saved\n",
      "Epoch 2, training accuracy: 0.960124\n",
      "Epoch 2, validation accuracy: 0.961347\n",
      "Epoch 2, validation error 0.107196 - session saved\n",
      "Epoch 3, training accuracy: 0.961666\n",
      "Epoch 3, validation accuracy: 0.961418\n",
      "Epoch 3, validation error 0.104961 - session saved\n",
      "Epoch 4, training accuracy: 0.962832\n",
      "Epoch 4, validation accuracy: 0.96282\n",
      "Epoch 4, validation error 0.102163 - session saved\n",
      "Epoch 5, training accuracy: 0.963649\n",
      "Epoch 5, validation accuracy: 0.963092\n",
      "Epoch 5, validation error 0.101458 - session saved\n",
      "Epoch 6, training accuracy: 0.96421\n",
      "Epoch 6, validation accuracy: 0.96228\n",
      "Epoch 7, training accuracy: 0.964582\n",
      "Epoch 7, validation accuracy: 0.963947\n",
      "Epoch 7, validation error 0.0991454 - session saved\n",
      "Epoch 8, training accuracy: 0.965075\n",
      "Epoch 8, validation accuracy: 0.962123\n",
      "Epoch 9, training accuracy: 0.965167\n",
      "Epoch 9, validation accuracy: 0.963993\n",
      "Epoch 9, validation error 0.0985859 - session saved\n",
      "Epoch 10, training accuracy: 0.965696\n",
      "Epoch 10, validation accuracy: 0.963706\n",
      "Epoch 11, training accuracy: 0.965807\n",
      "Epoch 11, validation accuracy: 0.963837\n",
      "Epoch 12, training accuracy: 0.965975\n",
      "Epoch 12, validation accuracy: 0.962366\n",
      "Epoch 13, training accuracy: 0.966108\n",
      "Epoch 13, validation accuracy: 0.963202\n",
      "Epoch 14, training accuracy: 0.966408\n",
      "Epoch 14, validation accuracy: 0.964082\n",
      "Epoch 15, training accuracy: 0.966671\n",
      "Epoch 15, validation accuracy: 0.963511\n",
      "Epoch 16, training accuracy: 0.966577\n",
      "Epoch 16, validation accuracy: 0.963096\n",
      "Epoch 17, training accuracy: 0.966906\n",
      "Epoch 17, validation accuracy: 0.964479\n",
      "Epoch 17, validation error 0.0984369 - session saved\n",
      "Epoch 18, training accuracy: 0.967062\n",
      "Epoch 18, validation accuracy: 0.96396\n",
      "Epoch 19, training accuracy: 0.967138\n",
      "Epoch 19, validation accuracy: 0.963623\n",
      "Epoch 20, training accuracy: 0.967311\n",
      "Epoch 20, validation accuracy: 0.965345\n",
      "Epoch 20, validation error 0.0961657 - session saved\n",
      "Epoch 21, training accuracy: 0.967473\n",
      "Epoch 21, validation accuracy: 0.96438\n",
      "Epoch 22, training accuracy: 0.967521\n",
      "Epoch 22, validation accuracy: 0.963036\n",
      "Epoch 23, training accuracy: 0.967564\n",
      "Epoch 23, validation accuracy: 0.964602\n",
      "Epoch 24, training accuracy: 0.967603\n",
      "Epoch 24, validation accuracy: 0.964954\n",
      "Epoch 25, training accuracy: 0.967776\n",
      "Epoch 25, validation accuracy: 0.964555\n",
      "Epoch 26, training accuracy: 0.967687\n",
      "Epoch 26, validation accuracy: 0.964285\n",
      "Epoch 27, training accuracy: 0.967897\n",
      "Epoch 27, validation accuracy: 0.964258\n",
      "Epoch 28, training accuracy: 0.967832\n",
      "Epoch 28, validation accuracy: 0.964637\n",
      "Epoch 29, training accuracy: 0.968003\n",
      "Epoch 29, validation accuracy: 0.963907\n",
      "Epoch 30, training accuracy: 0.96782\n",
      "Epoch 30, validation accuracy: 0.964768\n",
      "Training finished in epoch 30 due to early stopping.\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #Saver to save the best epoch (the one with lowest error)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #Visualize on Tensorboard\n",
    "    train_writer = tf.summary.FileWriter('./graphs/{}/train'.format(MODEL_NAME), sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter('./graphs/{}/validation'.format(MODEL_NAME), sess.graph)\n",
    "\n",
    "    #Prepare iterator for the dataset\n",
    "    training_iterator = training_dataset.make_one_shot_iterator()\n",
    "    next_training_batch = training_iterator.get_next()\n",
    "\n",
    "    validation_iterator = validation_dataset.make_one_shot_iterator()\n",
    "    next_validation_batch = validation_iterator.get_next()\n",
    "\n",
    "    print(\"Started training.\")\n",
    "    min_validation_loss = sys.maxint\n",
    "    epochs_without_improvement = 0 # to determine whether early stopping is reached\n",
    "    \n",
    "    for epoch_i in range(EPOCHS):\n",
    "        training_losses = []\n",
    "        training_accuracies = []\n",
    "        for i in range(training_steps):\n",
    "            batch = sess.run(next_training_batch)\n",
    "\n",
    "            accuracy_val, loss_val  = sess.run([accuracy, cross_entropy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "            training_accuracies.append(accuracy_val)\n",
    "            training_losses.append(loss_val)\n",
    "            \n",
    "            train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            \n",
    "        epoch_accuracy = np.mean(training_accuracies)\n",
    "        epoch_loss = np.mean(training_losses)\n",
    "        \n",
    "        print('Epoch %d, training accuracy: %g' % (epoch_i, epoch_accuracy))\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=\"accuracy\", simple_value=epoch_accuracy)\n",
    "        summary.value.add(tag=\"loss\", simple_value=epoch_loss)\n",
    "        train_writer.add_summary(summary, global_step=epoch_i)\n",
    "\n",
    "        validation_accuracies = []\n",
    "        validation_losses = []\n",
    "        for j in range(validation_steps):\n",
    "            batch = sess.run(next_validation_batch)\n",
    "\n",
    "            accuracy_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "            validation_accuracies.append(accuracy_val)\n",
    "            validation_losses.append(loss_val)\n",
    "            \n",
    "        epoch_accuracy = np.mean(validation_accuracies)\n",
    "        epoch_loss = np.mean(validation_losses)        \n",
    "\n",
    "        print('Epoch %d, validation accuracy: %g' % (epoch_i, epoch_accuracy))\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=\"accuracy\", simple_value=epoch_accuracy)\n",
    "        summary.value.add(tag=\"loss\", simple_value=epoch_loss)\n",
    "        validation_writer.add_summary(summary, global_step=epoch_i)\n",
    "\n",
    "        if(epoch_loss < min_validation_loss):\n",
    "            min_validation_loss = epoch_loss\n",
    "            epochs_without_improvement = 0\n",
    "            print('Epoch %d, validation loss %g - session saved' % (epoch_i, min_validation_loss))\n",
    "            saver.save(sess, \"./checkpoint/{}.ckpt\".format(MODEL_NAME))\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement == EARLY_STOPPING:\n",
    "                print('Training finished in epoch %d due to early stopping.' % epoch_i)\n",
    "                break\n",
    "    print('Finished training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/PeakLens_optimized.ckpt\n",
      "Started test.\n",
      "Test accuracy: 0.963755\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Restore the best session\n",
    "    saver.restore(sess, \"./checkpoint/{}.ckpt\".format(MODEL_NAME))\n",
    "    \n",
    "    # Compute test accuracy\n",
    "    test_accuracies = []\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    next_test_batch = test_iterator.get_next()\n",
    "    print(\"Started test.\")\n",
    "    for i in range(test_steps):\n",
    "        batch = sess.run(next_test_batch)\n",
    "        accuracy_val = sess.run([accuracy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "        test_accuracies.append(accuracy_val)\n",
    "\n",
    "    print('Test accuracy: %g' % np.mean(test_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/PeakLens_optimized.ckpt\n",
      "INFO:tensorflow:Froze 16 variables.\n",
      "INFO:tensorflow:Converted 16 variables to const ops.\n",
      "Converted model to pb.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define input size dimensions for complete images\n",
    "x = tf.placeholder(tf.float32, [1, 240, 320, 3])\n",
    "y_conv, softmax = deepnn(x)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if not os.path.exists('./protobufs/'):\n",
    "    os.makedir('./protobufs/')\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"./checkpoint/{}.ckpt\".format(MODEL_NAME))\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"softmax_tensor\"])\n",
    "    \n",
    "    with tf.gfile.GFile(\"./protobufs/{}.pb\".format(MODEL_NAME), \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "        print('Converted model to pb.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
