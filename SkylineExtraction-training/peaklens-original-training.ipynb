{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train PeakLens original model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"PeakLens_original\"\n",
    "DATASET_PATH = \"/data/skyline-extraction-patches-dataset\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "EARLY_STOPPING = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=x,\n",
    "        filters=20,\n",
    "        kernel_size=[6, 6],\n",
    "        name=\"conv1\")\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, name=\"pool1\")\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=50,\n",
    "        kernel_size=[5, 5],\n",
    "        name=\"conv2\")\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, name=\"pool2\")\n",
    "\n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=500,\n",
    "        kernel_size=[4, 4],\n",
    "        name=\"conv3\")\n",
    "\n",
    "    relu = tf.nn.relu(conv3, name=\"relu\")\n",
    "\n",
    "    conv4 = tf.layers.conv2d(\n",
    "        inputs=relu,\n",
    "        filters=2,\n",
    "        kernel_size=[1, 1],\n",
    "        name=\"conv4\")\n",
    "\n",
    "    output = tf.reshape(conv4, [-1, 1 * 1 * 2])\n",
    "\n",
    "    softmax = tf.nn.softmax(output, name=\"softmax_tensor\")\n",
    "\n",
    "    return output, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_label(path, label):\n",
    "    image = tf.read_file(path)\n",
    "    image_decoded = tf.image.decode_jpeg(image)\n",
    "    image_decoded = tf.image.random_flip_left_right(image_decoded)    \n",
    "    return image_decoded, label\n",
    "\n",
    "def get_dataset_split(split):\n",
    "    positive_paths = glob.glob(\"{}/{}/patches/positive/*.jpg\".format(DATASET_PATH, split))\n",
    "    negative_paths = glob.glob(\"{}/{}/patches/negative/*.jpg\".format(DATASET_PATH, split))\n",
    "  \n",
    "    positive_labels = [1] * len(positive_paths)\n",
    "    negative_labels = [0] * len(negative_paths)\n",
    "  \n",
    "    paths = positive_paths + negative_paths\n",
    "    labels = positive_labels + negative_labels\n",
    "\n",
    "    tf_paths = tf.constant(paths)\n",
    "    tf_labels = tf.constant(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tf_paths, tf_labels))\n",
    "    dataset = dataset.map(load_image_and_label, num_parallel_calls=multiprocessing.cpu_count())\\\n",
    "        .shuffle(len(paths)).batch(BATCH_SIZE).repeat(EPOCHS).prefetch(2)\n",
    "    return dataset, len(paths)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training split loaded (1,699,328 images).\n",
      "Validation split loaded (424,448 images).\n",
      "Test split loaded (530,944 images).\n"
     ]
    }
   ],
   "source": [
    "training_dataset, training_steps = get_dataset_split(\"training\")\n",
    "print(\"Training split loaded ({:,} images).\".format(training_steps*BATCH_SIZE))\n",
    "\n",
    "validation_dataset, validation_steps = get_dataset_split(\"validation\")\n",
    "print(\"Validation split loaded ({:,} images).\".format(validation_steps*BATCH_SIZE))\n",
    "\n",
    "test_dataset, test_steps = get_dataset_split(\"testing\")\n",
    "print(\"Test split loaded ({:,} images).\".format(test_steps*BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 428,732.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build the graph for the deep net\n",
    "x = tf.placeholder(tf.float32, [None, 29, 29, 3])\n",
    "y_ = tf.placeholder(tf.int64, [None])\n",
    "y_conv, softmax = deepnn(x)\n",
    "\n",
    "print(\"Parameters: {:,}.\".format(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss, adam_optimizer, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cross entropy as cost function\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y_conv)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Set Adam optimizer as trainer\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), y_)\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to visualize the training with tensorboard by executing the following command with the corresponding logdir path\n",
    "\n",
    "tensorboard --logdir=graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training.\n",
      "Epoch 0, training accuracy: 0.926798\n",
      "Epoch 0, validation accuracy: 0.939849\n",
      "Epoch 0, validation error 0.16793 - session saved\n",
      "Epoch 1, training accuracy: 0.945745\n",
      "Epoch 1, validation accuracy: 0.94463\n",
      "Epoch 1, validation error 0.154668 - session saved\n",
      "Epoch 2, training accuracy: 0.950902\n",
      "Epoch 2, validation accuracy: 0.945658\n",
      "Epoch 2, validation error 0.150339 - session saved\n",
      "Epoch 3, training accuracy: 0.954007\n",
      "Epoch 3, validation accuracy: 0.950819\n",
      "Epoch 3, validation error 0.137314 - session saved\n",
      "Epoch 4, training accuracy: 0.955894\n",
      "Epoch 4, validation accuracy: 0.954305\n",
      "Epoch 4, validation error 0.125156 - session saved\n",
      "Epoch 5, training accuracy: 0.957138\n",
      "Epoch 5, validation accuracy: 0.954136\n",
      "Epoch 6, training accuracy: 0.958547\n",
      "Epoch 6, validation accuracy: 0.954426\n",
      "Epoch 7, training accuracy: 0.959597\n",
      "Epoch 7, validation accuracy: 0.953667\n",
      "Epoch 8, training accuracy: 0.960534\n",
      "Epoch 8, validation accuracy: 0.955365\n",
      "Epoch 9, training accuracy: 0.961298\n",
      "Epoch 9, validation accuracy: 0.955684\n",
      "Epoch 10, training accuracy: 0.962162\n",
      "Epoch 10, validation accuracy: 0.956181\n",
      "Epoch 11, training accuracy: 0.962801\n",
      "Epoch 11, validation accuracy: 0.957901\n",
      "Epoch 11, validation error 0.121446 - session saved\n",
      "Epoch 12, training accuracy: 0.963492\n",
      "Epoch 12, validation accuracy: 0.956014\n",
      "Epoch 13, training accuracy: 0.964173\n",
      "Epoch 13, validation accuracy: 0.958188\n",
      "Epoch 13, validation error 0.120607 - session saved\n",
      "Epoch 14, training accuracy: 0.964824\n",
      "Epoch 14, validation accuracy: 0.956466\n",
      "Epoch 15, training accuracy: 0.965459\n",
      "Epoch 15, validation accuracy: 0.956382\n",
      "Epoch 16, training accuracy: 0.965985\n",
      "Epoch 16, validation accuracy: 0.95754\n",
      "Epoch 17, training accuracy: 0.966389\n",
      "Epoch 17, validation accuracy: 0.952716\n",
      "Epoch 18, training accuracy: 0.966891\n",
      "Epoch 18, validation accuracy: 0.958147\n",
      "Epoch 19, training accuracy: 0.967406\n",
      "Epoch 19, validation accuracy: 0.956658\n",
      "Epoch 20, training accuracy: 0.967985\n",
      "Epoch 20, validation accuracy: 0.9571\n",
      "Epoch 21, training accuracy: 0.968504\n",
      "Epoch 21, validation accuracy: 0.956858\n",
      "Epoch 22, training accuracy: 0.969076\n",
      "Epoch 22, validation accuracy: 0.956454\n",
      "Epoch 23, training accuracy: 0.969339\n",
      "Epoch 23, validation accuracy: 0.955104\n",
      "Training finished in epoch 23 due to early stopping.\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #Saver to save the best epoch (the one with lowest error)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #Visualize on Tensorboard\n",
    "    train_writer = tf.summary.FileWriter('./graphs/{}/train'.format(MODEL_NAME), sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter('./graphs/{}/validation'.format(MODEL_NAME), sess.graph)\n",
    "\n",
    "    #Prepare iterator for the dataset\n",
    "    training_iterator = training_dataset.make_one_shot_iterator()\n",
    "    next_training_batch = training_iterator.get_next()\n",
    "\n",
    "    validation_iterator = validation_dataset.make_one_shot_iterator()\n",
    "    next_validation_batch = validation_iterator.get_next()\n",
    "\n",
    "    print(\"Started training.\")\n",
    "    min_validation_loss = sys.maxint\n",
    "    epochs_without_improvement = 0 # to determine whether early stopping is reached\n",
    "    \n",
    "    for epoch_i in range(EPOCHS):\n",
    "        training_losses = []\n",
    "        training_accuracies = []\n",
    "        for i in range(training_steps):\n",
    "            batch = sess.run(next_training_batch)\n",
    "\n",
    "            accuracy_val, loss_val  = sess.run([accuracy, cross_entropy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "            training_accuracies.append(accuracy_val)\n",
    "            training_losses.append(loss_val)\n",
    "            \n",
    "            train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            \n",
    "        epoch_accuracy = np.mean(training_accuracies)\n",
    "        epoch_loss = np.mean(training_losses)\n",
    "        \n",
    "        print('Epoch %d, training accuracy: %g' % (epoch_i, epoch_accuracy))\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=\"accuracy\", simple_value=epoch_accuracy)\n",
    "        summary.value.add(tag=\"loss\", simple_value=epoch_loss)\n",
    "        train_writer.add_summary(summary, global_step=epoch_i)\n",
    "\n",
    "        validation_accuracies = []\n",
    "        validation_losses = []\n",
    "        for j in range(validation_steps):\n",
    "            batch = sess.run(next_validation_batch)\n",
    "\n",
    "            accuracy_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "            validation_accuracies.append(accuracy_val)\n",
    "            validation_losses.append(loss_val)\n",
    "            \n",
    "        epoch_accuracy = np.mean(validation_accuracies)\n",
    "        epoch_loss = np.mean(validation_losses)        \n",
    "\n",
    "        print('Epoch %d, validation accuracy: %g' % (epoch_i, epoch_accuracy))\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=\"accuracy\", simple_value=epoch_accuracy)\n",
    "        summary.value.add(tag=\"loss\", simple_value=epoch_loss)\n",
    "        validation_writer.add_summary(summary, global_step=epoch_i)\n",
    "\n",
    "        if(epoch_loss < min_validation_loss):\n",
    "            min_validation_loss = epoch_loss\n",
    "            epochs_without_improvement = 0\n",
    "            print('Epoch %d, validation loss %g - session saved' % (epoch_i, min_validation_loss))\n",
    "            saver.save(sess, \"./checkpoint/{}.ckpt\".format(MODEL_NAME))\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement == EARLY_STOPPING:\n",
    "                print('Training finished in epoch %d due to early stopping.' % epoch_i)\n",
    "                break\n",
    "    print('Finished training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/PeakLens_original.ckpt\n",
      "Started test.\n",
      "Test accuracy: 0.956606\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Restore the best session\n",
    "    saver.restore(sess, \"./checkpoint/{}.ckpt\".format(MODEL_NAME))\n",
    "    \n",
    "    # Compute test accuracy\n",
    "    test_accuracies = []\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    next_test_batch = test_iterator.get_next()\n",
    "    print(\"Started test.\")\n",
    "    for i in range(test_steps):\n",
    "        batch = sess.run(next_test_batch)\n",
    "        accuracy_val = sess.run([accuracy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "        test_accuracies.append(accuracy_val)\n",
    "\n",
    "    print('Test accuracy: %g' % np.mean(test_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/PeakLens_original.ckpt\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "Converted model to pb.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define input size dimensions for complete images\n",
    "x = tf.placeholder(tf.float32, [1, 240, 320, 3])\n",
    "y_conv, softmax = deepnn(x)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists('./protobufs/'):\n",
    "    os.makedir('./protobufs/')\n",
    "    \n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"./checkpoint/{}.ckpt\".format(MODEL_NAME))\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"softmax_tensor\"])\n",
    "    \n",
    "    with tf.gfile.GFile(\"./protobufs/{}.pb\".format(MODEL_NAME), \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "        print('Converted model to pb.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
